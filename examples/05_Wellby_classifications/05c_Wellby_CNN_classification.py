# -*- coding: utf-8 -*-
"""Wellby_CNN_run.ipynb

Wellby Dataset Stress and Fatigue Classification (End-to-end CNN) Evaluation
Tables VII and VIII in the paper
10.1109/TAFFC.2025.3628467

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gcg9rzUORLn5PyGSMzsKylWCNJ9jBU3s
"""

# This code was run in Google Colab using the below methods to classify 
# stress and fatigue in the Wellby dataset using end-to-end CNN.
# Justin Laiti - August 24th 2025

# ===== WELLBY END-TO-END CNN FOR GOOGLE COLAB =====

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, balanced_accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import gc
from tqdm import tqdm
import os

# ===== SETUP AND GPU CHECK =====
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

class WellbyDilatedCNN(nn.Module):
    """
    Dilated CNN adapted for Wellby (smaller dataset, more regularization)
    """
    def __init__(self, input_length=2900, num_classes=2):
        super(WellbyDilatedCNN, self).__init__()

        # Same dilation progression as WESAD/AKTIVES but more dropout
        self.conv_blocks = nn.ModuleList([
            # Block 1: dilation=1, capture fine details
            nn.Sequential(
                nn.Conv1d(1, 64, kernel_size=7, dilation=1, padding='same'),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Dropout(0.4)  # Increased from 0.2
            ),
            # Block 2: dilation=2
            nn.Sequential(
                nn.Conv1d(64, 128, kernel_size=7, dilation=2, padding='same'),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.5)  # Increased from 0.3
            ),
            # Block 3: dilation=4
            nn.Sequential(
                nn.Conv1d(128, 256, kernel_size=7, dilation=4, padding='same'),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.6)  # Increased from 0.3
            ),
            # Block 4: dilation=8
            nn.Sequential(
                nn.Conv1d(256, 256, kernel_size=7, dilation=8, padding='same'),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.7)  # Increased from 0.4
            ),
            # Block 5: dilation=16
            nn.Sequential(
                nn.Conv1d(256, 512, kernel_size=7, dilation=16, padding='same'),
                nn.BatchNorm1d(512),
                nn.ReLU(),
                nn.Dropout(0.7)  # Increased from 0.4
            )
        ])

        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.8),  # Very high dropout for tiny dataset
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        for block in self.conv_blocks:
            x = block(x)

        x = self.global_pool(x)
        x = self.classifier(x)
        return x

class PPGDataset(Dataset):
    """Dataset class for Wellby PPG data"""
    def __init__(self, windows, labels):
        self.windows = torch.FloatTensor(windows)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.windows)

    def __getitem__(self, idx):
        return self.windows[idx].unsqueeze(0), self.labels[idx]

def load_wellby_data_cnn(task='sleep', skip_start_samples=101, fs=50):
    """
    Load Wellby data for CNN training (adapted from hybrid approach)

    Args:
        task: 'stress' or 'sleep'
        skip_start_samples: Number of initial samples to skip (noise reduction)
        fs: Sampling frequency

    Returns:
        signals, labels, participants, session_ids
    """
    print(f"Loading Wellby data for {task} CNN classification...")

    # Look for data files in common Colab locations
    possible_paths = [
        "./data/Wellby/",
        "./Wellby/",
        "./",
    ]

    data_dir = None
    for path in possible_paths:
        ppg_path = os.path.join(path, "selected_ppg_data.csv")
        info_path = os.path.join(path, "Wellby_all_subjects_features.csv")
        if os.path.exists(ppg_path) and os.path.exists(info_path):
            data_dir = path
            break

    if data_dir is None:
        raise ValueError("Could not find Wellby data files! Upload selected_ppg_data.csv and Wellby_all_subjects_features.csv")

    print(f"Found Wellby data in: {data_dir}")

    # Load PPG signals (transposed format - sessions as columns)
    ppg_data = pd.read_csv(os.path.join(data_dir, "selected_ppg_data.csv"))
    print(f"PPG data shape: {ppg_data.shape}")

    # Load session information
    session_info = pd.read_csv(os.path.join(data_dir, "Wellby_all_subjects_features.csv"))
    print(f"Session info shape: {session_info.shape}")

    # Get label column based on task
    label_col = f'{task}_binary'
    if label_col not in session_info.columns:
        raise ValueError(f"Label column '{label_col}' not found! Available: {session_info.columns.tolist()}")

    # Find common session IDs
    ppg_session_ids = ppg_data.columns.tolist()
    info_session_ids = session_info['Session_ID'].tolist()
    common_sessions = list(set(ppg_session_ids) & set(info_session_ids))

    print(f"PPG sessions: {len(ppg_session_ids)}")
    print(f"Info sessions: {len(info_session_ids)}")
    print(f"Common sessions: {len(common_sessions)}")

    if len(common_sessions) == 0:
        raise ValueError("No matching session IDs found!")

    # Process each session
    all_signals = []
    all_labels = []
    all_participants = []
    all_session_ids = []
    skipped_sessions = 0

    # Determine target length after skipping start samples
    valid_lengths = []
    for session_id in common_sessions:
        ppg_signal = ppg_data[session_id].dropna().values
        if len(ppg_signal) > skip_start_samples:
            valid_lengths.append(len(ppg_signal) - skip_start_samples)

    if len(valid_lengths) == 0:
        raise ValueError("All signals are too short after skipping start samples!")

    target_length = min(valid_lengths)
    print(f"Target signal length after skipping first {skip_start_samples} samples: {target_length}")
    print(f"Signal duration: ~{target_length/fs:.1f} seconds")

    for session_id in tqdm(common_sessions, desc="Processing signals"):
        try:
            # Get PPG signal
            ppg_signal = ppg_data[session_id].dropna().values

            # Skip if too short
            if len(ppg_signal) <= skip_start_samples:
                skipped_sessions += 1
                continue

            # Skip initial noisy samples
            trimmed_signal = ppg_signal[skip_start_samples:]

            # Truncate to target length
            if len(trimmed_signal) >= target_length:
                processed_signal = trimmed_signal[:target_length]
            else:
                # Zero pad if needed
                padding = target_length - len(trimmed_signal)
                processed_signal = np.pad(trimmed_signal, (0, padding), mode='constant', constant_values=0)

            # Clean signal (remove NaN)
            clean_signal = processed_signal[~np.isnan(processed_signal)]
            if len(clean_signal) == 0:
                skipped_sessions += 1
                continue

            # Standardize (matching hybrid approach)
            signal_mean = np.mean(clean_signal)
            signal_std = np.std(clean_signal)
            if signal_std == 0:
                signal_std = 1e-8
            standardized_signal = (clean_signal - signal_mean) / signal_std

            # Final normalization for CNN
            final_signal = (standardized_signal - np.mean(standardized_signal)) / (np.std(standardized_signal) + 1e-8)

            # Get session info
            session_row = session_info[session_info['Session_ID'] == session_id].iloc[0]
            participant = session_row['Participant']
            label = session_row[label_col]

            # Store processed data
            all_signals.append(final_signal)
            all_labels.append(int(label))
            all_participants.append(participant)
            all_session_ids.append(session_id)

        except Exception as e:
            print(f"Error processing session {session_id}: {e}")
            skipped_sessions += 1

    print(f"Successfully processed: {len(all_signals)} sessions")
    print(f"Skipped sessions: {skipped_sessions}")
    print(f"Label distribution: {np.bincount(all_labels)}")
    print(f"Unique participants: {len(np.unique(all_participants))}")

    return np.array(all_signals), np.array(all_labels), np.array(all_participants), np.array(all_session_ids)

def train_wellby_cnn_fold(model, train_loader, device, epochs=25, patience=15):
    """
    Train CNN for one fold (adapted for tiny Wellby dataset)
    """
    # Calculate class weights
    all_labels = []
    for _, labels in train_loader:
        all_labels.extend(labels.numpy())

    if len(np.unique(all_labels)) > 1:
        class_counts = np.bincount(all_labels)
        total_samples = len(all_labels)
        class_weights = total_samples / (len(class_counts) * class_counts)
        class_weights_tensor = torch.FloatTensor(class_weights).to(device)
    else:
        class_weights_tensor = None

    # Setup training with strong regularization
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)  # Lower LR, higher weight decay
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=8
    )

    best_loss = float('inf')
    patience_counter = 0

    model.train()

    for epoch in range(epochs):
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0

        for batch_windows, batch_labels in train_loader:
            batch_windows = batch_windows.to(device, non_blocking=True)
            batch_labels = batch_labels.to(device, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(batch_windows)
            loss = criterion(outputs, batch_labels)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            epoch_total += batch_labels.size(0)
            epoch_correct += (predicted == batch_labels).sum().item()

        avg_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else float('inf')
        epoch_acc = 100 * epoch_correct / epoch_total if epoch_total > 0 else 0

        scheduler.step(avg_loss)

        # Early stopping
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            # Save best model for this fold
            torch.save(model.state_dict(), 'temp_best_wellby_cnn.pth')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                break

        torch.cuda.empty_cache()

    # Load best model
    model.load_state_dict(torch.load('temp_best_wellby_cnn.pth'))
    return model

def run_wellby_cnn_cv(signals, labels, participants, task='sleep'):
    """
    Run CNN with 3-fold cross-validation (matching hybrid evaluation)
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    print(f"Total data: {len(signals)} sessions from {len(np.unique(participants))} participants")
    print(f"Signal shape: {signals[0].shape}")
    print(f"Label distribution: {np.bincount(labels)}")

    # 3-fold stratified group CV (same as hybrid approach)
    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)

    fold_results = []

    for fold_idx, (train_idx, test_idx) in enumerate(cv.split(signals, labels, participants)):
        print(f"\n=== Fold {fold_idx + 1}/3 ===")

        X_train, X_test = signals[train_idx], signals[test_idx]
        y_train, y_test = labels[train_idx], labels[test_idx]

        print(f"Train: {len(X_train)} samples, distribution: {np.bincount(y_train)}")
        print(f"Test: {len(X_test)} samples, distribution: {np.bincount(y_test)}")

        # Create datasets and loaders (very small batches)
        train_dataset = PPGDataset(X_train, y_train)
        test_dataset = PPGDataset(X_test, y_test)

        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True)
        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, pin_memory=True)

        # Initialize model for this fold
        model = WellbyDilatedCNN(input_length=len(signals[0])).to(device)

        # Train
        print(f"Training fold {fold_idx + 1}...")
        start_time = time.time()
        model = train_wellby_cnn_fold(model, train_loader, device)
        train_time = time.time() - start_time

        # Evaluate
        print(f"Evaluating fold {fold_idx + 1}...")
        model.eval()
        test_preds = []
        test_probs = []
        test_true = []

        with torch.no_grad():
            for batch_windows, batch_labels in test_loader:
                batch_windows = batch_windows.to(device)
                outputs = model(batch_windows)
                probs = torch.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)

                test_preds.extend(preds.cpu().numpy())
                test_probs.extend(probs.cpu().numpy())
                test_true.extend(batch_labels.numpy())

        # Calculate metrics (matching hybrid format)
        test_preds = np.array(test_preds)
        test_probs = np.array(test_probs)
        test_true = np.array(test_true)

        accuracy = accuracy_score(test_true, test_preds) * 100
        balanced_acc = balanced_accuracy_score(test_true, test_preds) * 100
        f1 = f1_score(test_true, test_preds, average='weighted') * 100

        # AUC and Average Precision
        if len(np.unique(test_true)) > 1:
            auc = roc_auc_score(test_true, test_probs[:, 1]) * 100
            from sklearn.metrics import average_precision_score
            avg_precision = average_precision_score(test_true, test_probs[:, 1]) * 100
        else:
            auc = 0.0
            avg_precision = 0.0

        fold_result = {
            'fold': fold_idx + 1,
            'accuracy': accuracy,
            'balanced_accuracy': balanced_acc,
            'auc_roc': auc,
            'avg_precision': avg_precision,
            'f1_score': f1,
            'train_time': train_time
        }

        fold_results.append(fold_result)

        print(f"Fold {fold_idx + 1} Results:")
        print(f"  Accuracy: {accuracy:.2f}%")
        print(f"  Balanced Acc: {balanced_acc:.2f}%")
        print(f"  AUC-ROC: {auc:.2f}%")
        print(f"  Avg Precision: {avg_precision:.2f}%")
        print(f"  F1-Score: {f1:.2f}%")
        print(f"  Train Time: {train_time:.1f}s")

        torch.cuda.empty_cache()

    return fold_results

def save_wellby_cnn_results(fold_results, task):
    """
    Save results in format matching hybrid evaluation
    """
    # Calculate means and stds across folds
    metrics = ['accuracy', 'balanced_accuracy', 'auc_roc', 'avg_precision', 'f1_score']
    results_summary = {}

    for metric in metrics:
        values = [fold[metric] for fold in fold_results]
        results_summary[metric] = {
            'mean': np.mean(values),
            'std': np.std(values),
            'formatted': f"{np.mean(values):.2f} Â± {np.std(values):.2f}"
        }

    # Create results DataFrame (matching hybrid format)
    results_df = pd.DataFrame([{
        'Model': 'End-to-End CNN',
        'Average Precision (AUPOC)': results_summary['avg_precision']['formatted'],
        'AUC': results_summary['auc_roc']['formatted'],
        'Balanced Accuracy': results_summary['balanced_accuracy']['formatted'],
        'Best Parameters': 'Dilated CNN with 3-fold CV'
    }])

    print(f"\n=== WELLBY {task.upper()} CNN RESULTS (3-Fold CV) ===")
    print(results_df.to_string(index=False))

    # Save detailed results
    fold_df = pd.DataFrame(fold_results)
    fold_df.to_csv(f'wellby_{task}_cnn_detailed_results.csv', index=False)
    results_df.to_csv(f'wellby_{task}_cnn_summary_results.csv', index=False)

    print(f"\nDetailed results saved to: wellby_{task}_cnn_detailed_results.csv")
    print(f"Summary results saved to: wellby_{task}_cnn_summary_results.csv")

    # Clean up temp file
    if os.path.exists('temp_best_wellby_cnn.pth'):
        os.remove('temp_best_wellby_cnn.pth')

    return results_df, fold_df

# ===== MAIN EXECUTION =====
def main():
    print("=== WELLBY END-TO-END CNN CLASSIFICATION ===")
    print("Using raw PPG signals with dilated CNN architecture")

    # Configuration
    TASK = 'sleep'  # 'sleep' or 'stress' classification

    print(f"\nTask: {TASK}")

    try:
        # Load data
        signals, labels, participants, session_ids = load_wellby_data_cnn(task=TASK)

        if len(signals) == 0:
            print("No data found! Upload Wellby CSV files.")
            return None, None

        print(f"\nDataset summary:")
        print(f"  Total sessions: {len(signals)}")
        print(f"  Signal length: {signals[0].shape}")
        print(f"  Participants: {len(np.unique(participants))}")
        print(f"  Label distribution: {dict(zip(*np.unique(labels, return_counts=True)))}")

        # Calculate model parameters
        model = WellbyDilatedCNN(input_length=len(signals[0]))
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  Model parameters: {total_params:,}")
        print(f"  Samples per parameter: {len(signals)/total_params:.4f}")

        # Run CNN evaluation
        print(f"\nRunning 3-fold cross-validation...")
        fold_results = run_wellby_cnn_cv(signals, labels, participants, task=TASK)

        # Save results
        summary_df, detailed_df = save_wellby_cnn_results(fold_results, TASK)

        print(f"\n=== CNN PIPELINE COMPLETE ===")

        return summary_df, detailed_df

    except Exception as e:
        print(f"Error in main execution: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# ===== RUN FOR BOTH TASKS =====
if __name__ == "__main__":
    print("Upload your Wellby data files:")
    print("- selected_ppg_data.csv")
    print("- Wellby_all_subjects_features.csv")

    # Run for stress
    print("\n" + "="*50)
    print("RUNNING STRESS CLASSIFICATION")
    print("="*50)
    stress_summary, stress_detailed = main()

    # Run for sleep (change TASK in main() function)
    print("\n" + "="*50)
    print("RUNNING SLEEP CLASSIFICATION")
    print("="*50)
    print("Change TASK = 'sleep' in main() and run again")