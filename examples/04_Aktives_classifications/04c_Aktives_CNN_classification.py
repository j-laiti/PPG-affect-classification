# -*- coding: utf-8 -*-
"""AKTIVES_CNN_run.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDBPkkPU13an8tG4tgfH1H4-ZJivD4zY
"""

# This code was run in Google Colab using the below methods to classify 
# stress and fatigue in the Aktives dataset using end-to-end CNN.
# Justin Laiti - August 24th 2025

# ===== MAIN CODE =====

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import gc
from tqdm import tqdm

class FixedDilatedCNN(nn.Module):
    """
    Same CNN as WESAD - only input_length parameter changed
    """
    def __init__(self, input_length=1920, num_classes=2):
        super(FixedDilatedCNN, self).__init__()

        # Same dilation progression as WESAD
        self.conv_blocks = nn.ModuleList([
            # Block 1: dilation=1, capture fine details
            nn.Sequential(
                nn.Conv1d(1, 32, kernel_size=5, dilation=1, padding='same'),
                nn.BatchNorm1d(32),
                nn.ReLU(),
                nn.Dropout(0.4)
            ),
            # Block 2: dilation=2
            nn.Sequential(
                nn.Conv1d(32, 64, kernel_size=5, dilation=2, padding='same'),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Dropout(0.5)
            ),
            # Block 3: dilation=4
            nn.Sequential(
                nn.Conv1d(64, 128, kernel_size=5, dilation=4, padding='same'),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.6)
            ),
            # Block 4: dilation=8
            nn.Sequential(
                nn.Conv1d(128, 256, kernel_size=5, dilation=8, padding='same'),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.7)
            ),
            # Block 5: dilation=16 (EXACT same as WESAD)
            nn.Sequential(
                nn.Conv1d(256, 512, kernel_size=7, dilation=16, padding='same'),
                nn.BatchNorm1d(512),
                nn.ReLU(),
                nn.Dropout(0.7)
            )
        ])

        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.8),
            nn.Linear(128, num_classes)  # Raw logits for CrossEntropyLoss
        )

    def forward(self, x):
        for block in self.conv_blocks:
            x = block(x)

        # Global pooling and classification
        x = self.global_pool(x)
        x = self.classifier(x)
        return x

class PPGDataset(Dataset):
    """Optimized Dataset class for GPU training"""
    def __init__(self, windows, labels):
        self.windows = torch.FloatTensor(windows)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.windows)

    def __getitem__(self, idx):
        return self.windows[idx].unsqueeze(0), self.labels[idx]

def load_all_aktives_windows_colab():
    """
    Load all AKTIVES analysis windows from all cohorts (Colab version)
    """
    print("Loading all AKTIVES analysis windows...")

    # Look for analysis windows in common locations
    possible_paths = [
        "./data/Aktives/analysis_windows/",
        "./Aktives/analysis_windows/",
        "./analysis_windows/",
        "./"
    ]

    windows_dir = None
    for path in possible_paths:
        if os.path.exists(path):
            windows_dir = Path(path)
            break

    if windows_dir is None:
        # Try to find it recursively
        for root, dirs, files in os.walk("."):
            if "analysis_windows" in dirs:
                windows_dir = Path(root) / "analysis_windows"
                break

    if windows_dir is None:
        raise ValueError("Could not find analysis_windows directory!")

    print(f"Found analysis windows directory: {windows_dir}")

    cohorts = ['dyslexia', 'ID', 'OBPI', 'TD']
    all_windows = []

    for cohort in cohorts:
        window_file = windows_dir / f"analysis_windows_{cohort}.csv"

        if window_file.exists():
            windows_df = pd.read_csv(window_file)
            windows_df['cohort'] = cohort
            all_windows.append(windows_df)
            print(f"  Loaded {cohort}: {len(windows_df)} windows")
        else:
            print(f"  Warning: {window_file} not found")

    if not all_windows:
        raise ValueError("No analysis windows found!")

    combined_windows = pd.concat(all_windows, ignore_index=True)
    print(f"Total windows loaded: {len(combined_windows)}")

    return combined_windows

def load_ppg_for_window_colab(row, target_length=1920, fs=64):
    """
    Load raw PPG data for a specific window (Colab version)
    Matches your hybrid approach but returns raw signal for CNN
    """
    try:
        # Extract window information
        participant_cell = row["Participant"]
        participant = participant_cell.split("_")[0]
        game = participant_cell.split("_")[1]

        interval_start = row["Interval_Start"]
        interval_end = row["Interval_End"]
        cohort = row["cohort"]

        # Map cohort names to folder names
        cohort_folder_map = {
            'dyslexia': 'Dyslexia',
            'ID': 'Intellectual Disabilities',
            'OBPI': 'Obstetric Brachial Plexus Injuries',
            'TD': 'Typically Developed'
        }

        cohort_folder = cohort_folder_map[cohort]

        # Look for PPG data in common locations
        possible_ppg_paths = [
            f"./data/Aktives/PPG/{cohort_folder}/{participant}/{game}/BVP.csv",
            f"./Aktives/PPG/{cohort_folder}/{participant}/{game}/BVP.csv",
            f"./PPG/{cohort_folder}/{participant}/{game}/BVP.csv"
        ]

        ppg_file_path = None
        for path in possible_ppg_paths:
            if os.path.exists(path):
                ppg_file_path = path
                break

        if ppg_file_path is None:
            return None, None

        # Load PPG data
        ppg_data = pd.read_csv(ppg_file_path)

        # Add time column
        ppg_data["Time"] = ppg_data.index / fs

        # Clean and convert values
        ppg_data['values'] = ppg_data['values'].astype(str).str.replace(',', '.', regex=False).astype(float)

        # Select the interval
        ppg_interval = ppg_data[(ppg_data['Time'] >= interval_start) &
                               (ppg_data['Time'] <= interval_end)]

        if len(ppg_interval) == 0:
            return None, None

        raw_ppg_values = ppg_interval['values'].values

        # Check for sufficient data
        if len(raw_ppg_values) < fs * 10:  # Less than 10 seconds
            return None, None

        # Minimal preprocessing for CNN (same as hybrid approach)
        clean_ppg_values = raw_ppg_values[~np.isnan(raw_ppg_values)]
        if len(clean_ppg_values) == 0:
            return None, None

        # Basic standardization only (no filtering for CNN)
        ppg_mean = np.mean(clean_ppg_values)
        ppg_std = np.std(clean_ppg_values)
        if ppg_std == 0:
            ppg_std = 1e-8
        ppg_standardized = (clean_ppg_values - ppg_mean) / ppg_std

        # Handle variable length windows - pad or truncate to target_length
        if len(ppg_standardized) > target_length:
            processed_signal = ppg_standardized[:target_length]
        elif len(ppg_standardized) < target_length:
            padding = target_length - len(ppg_standardized)
            processed_signal = np.pad(ppg_standardized, (0, padding), mode='constant', constant_values=0)
        else:
            processed_signal = ppg_standardized

        # Additional normalization for CNN (matching hybrid approach)
        final_signal = (processed_signal - np.mean(processed_signal)) / (np.std(processed_signal) + 1e-8)

        return final_signal, row['Label']

    except Exception as e:
        print(f"Error loading PPG for window: {str(e)}")
        return None, None

def load_aktives_data_colab():
    """
    Load all AKTIVES data for CNN training
    """
    print("Loading AKTIVES data for CNN training...")

    # Load analysis windows
    all_windows = load_all_aktives_windows_colab()

    all_signals = []
    all_labels = []
    all_participants = []
    all_cohorts = []

    print(f"Processing {len(all_windows)} windows...")

    for idx, row in tqdm(all_windows.iterrows(), total=len(all_windows), desc="Loading PPG data"):
        signal, label = load_ppg_for_window_colab(row)

        if signal is not None:
            participant = row["Participant"].split("_")[0]
            all_signals.append(signal)
            all_labels.append(int(label))
            all_participants.append(participant)
            all_cohorts.append(row['cohort'])

    print(f"Successfully loaded {len(all_signals)} windows")
    print(f"Label distribution: {np.bincount(all_labels)}")
    print(f"Participants: {len(np.unique(all_participants))}")
    print(f"Cohorts: {np.unique(all_cohorts)}")

    return np.array(all_signals), np.array(all_labels), np.array(all_participants), np.array(all_cohorts)

def train_dilated_cnn_aktives(model, train_loader, device, epochs=30, patience=10):
    """
    Training adapted for AKTIVES (smaller dataset, more patience)
    """
    # Calculate class weights
    all_labels = []
    for _, labels in train_loader:
        all_labels.extend(labels.numpy())

    class_counts = np.bincount(all_labels)
    total_samples = len(all_labels)
    class_weights = total_samples / (len(class_counts) * class_counts)
    class_weights_tensor = torch.FloatTensor(class_weights).to(device)

    print(f"  Class distribution: {class_counts}")
    print(f"  Class weights: {class_weights}")

    # Setup training
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.Adam(model.parameters(), lr=0.0008, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )

    best_loss = float('inf')
    patience_counter = 0

    model.train()

    for epoch in range(epochs):
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0

        for batch_windows, batch_labels in train_loader:
            batch_windows = batch_windows.to(device, non_blocking=True)
            batch_labels = batch_labels.to(device, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(batch_windows)
            loss = criterion(outputs, batch_labels)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            epoch_total += batch_labels.size(0)
            epoch_correct += (predicted == batch_labels).sum().item()

        avg_loss = epoch_loss / len(train_loader)
        epoch_acc = 100 * epoch_correct / epoch_total

        scheduler.step(avg_loss)

        # Print progress every 10 epochs
        if (epoch + 1) % 10 == 0:
            print(f'  Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Acc={epoch_acc:.2f}%')

        # Early stopping
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'best_aktives_cnn.pth')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'  Early stopping at epoch {epoch+1}')
                break

        torch.cuda.empty_cache()

    # Load best model
    model.load_state_dict(torch.load('best_aktives_cnn.pth'))
    return model

def run_aktives_cnn_70_30(signals, labels, participants):
    """
    Run CNN with 70/30 split (matching your hybrid evaluation)
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    print(f"Total data: {len(signals)} windows from {len(np.unique(participants))} participants")

    # 70/30 split by participants (same as hybrid)
    unique_participants = np.unique(participants)
    train_participants, test_participants = train_test_split(
        unique_participants, test_size=0.3, random_state=42
    )

    print(f"Training participants ({len(train_participants)}): {sorted(train_participants)}")
    print(f"Test participants ({len(test_participants)}): {sorted(test_participants)}")

    # Create train/test splits
    train_mask = np.isin(participants, train_participants)
    test_mask = np.isin(participants, test_participants)

    X_train, X_test = signals[train_mask], signals[test_mask]
    y_train, y_test = labels[train_mask], labels[test_mask]

    print(f"\nData split:")
    print(f"Train: {len(X_train)} samples, distribution: {np.bincount(y_train)}")
    print(f"Test: {len(X_test)} samples, distribution: {np.bincount(y_test)}")

    # Create datasets and loaders
    train_dataset = PPGDataset(X_train, y_train)
    test_dataset = PPGDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,
                             num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,
                            num_workers=2, pin_memory=True)

    # Initialize model (EXACT same as WESAD, just different input length)
    model = FixedDilatedCNN(input_length=1920).to(device)  # Only change: 1920 vs 7680

    print(f"\nModel architecture:")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,}")

    # Train
    print(f"\nTraining CNN...")
    start_time = time.time()
    model = train_dilated_cnn_aktives(model, train_loader, device, epochs=30, patience=15)
    train_time = time.time() - start_time

    # Test
    print(f"\nEvaluating on test set...")
    model.eval()
    test_preds = []
    test_probs = []
    test_true = []

    with torch.no_grad():
        for batch_windows, batch_labels in test_loader:
            batch_windows = batch_windows.to(device)
            outputs = model(batch_windows)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            test_preds.extend(preds.cpu().numpy())
            test_probs.extend(probs.cpu().numpy())
            test_true.extend(batch_labels.numpy())

    # Calculate metrics (matching hybrid format)
    test_preds = np.array(test_preds)
    test_probs = np.array(test_probs)
    test_true = np.array(test_true)

    accuracy = accuracy_score(test_true, test_preds) * 100
    f1 = f1_score(test_true, test_preds, average='weighted') * 100

    if len(np.unique(test_true)) > 1:
        auc = roc_auc_score(test_true, test_probs[:, 1]) * 100  # Use probabilities for AUC
    else:
        auc = 0.0

    results = {
        'model': 'Dilated CNN',
        'accuracy': accuracy,
        'auc_roc': auc,
        'f1_score': f1,
        'train_time': train_time,
        'total_params': total_params
    }

    return results

def save_aktives_cnn_results(results):
    """Save results in format compatible with hybrid comparison"""

    print(f"\n=== AKTIVES CNN Results (70/30 Split) ===")
    print(f"Model: {results['model']}")
    print(f"Accuracy: {results['accuracy']:.2f}%")
    print(f"AUC-ROC:  {results['auc_roc']:.2f}%")
    print(f"F1-Score: {results['f1_score']:.2f}%")
    print(f"Training Time: {results['train_time']:.1f}s")
    print(f"Parameters: {results['total_params']:,}")

    # Save to CSV for comparison
    results_df = pd.DataFrame([results])
    results_df.to_csv('aktives_cnn_results.csv', index=False)
    print(f"\nResults saved to 'aktives_cnn_results.csv'")

    return results_df

# ===== MAIN EXECUTION =====
def main():
    print("=== AKTIVES End-to-End CNN Classification ===")
    print("Using raw PPG signals with dilated CNN architecture")

    try:
        # Load data
        signals, labels, participants, cohorts = load_aktives_data_colab()

        if len(signals) == 0:
            print("No data found! Check your uploaded files.")
            return

        print(f"\nDataset summary:")
        print(f"  Total windows: {len(signals)}")
        print(f"  Signal length: {signals[0].shape}")
        print(f"  Participants: {len(np.unique(participants))}")
        print(f"  Cohorts: {np.unique(cohorts)}")
        print(f"  Label distribution: {dict(zip(*np.unique(labels, return_counts=True)))}")

        # Run CNN evaluation
        cnn_results = run_aktives_cnn_70_30(signals, labels, participants)

        # Save results
        results_df = save_aktives_cnn_results(cnn_results)

        print("\n=== CNN Pipeline Complete ===")

        return results_df

    except Exception as e:
        print(f"Error in main execution: {e}")
        import traceback
        traceback.print_exc()
        return None

# Run the main function
if __name__ == "__main__":
    results = main()