# -*- coding: utf-8 -*-
"""WESAD_CNN_run.ipynb
LOSO Cross-Validation for WESAD Stress Classification
Outlined in Figure 5 and Table V of the paper
10.1109/TAFFC.2025.3628467

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nor7gnbWTmzWFDrDJPnf1SKztOnJKEei
"""

# This code was run in Google Colab using the below methods to classify 
# stress and fatigue in the WESAD dataset using end-to-end CNN.
# Justin Laiti - August 24th 2025

# ===== MAIN CODE =====

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import time
import gc

class FixedDilatedCNN(nn.Module):
    """
    Improved Dilated CNN fixing issues from Motaman et al.
    - Reasonable dilation rates for 120s windows
    - Proper output layer for CrossEntropyLoss
    - More efficient architecture
    """
    def __init__(self, input_length=7680, num_classes=2):
        super(FixedDilatedCNN, self).__init__()

        # More reasonable dilation progression
        # For 7680 length signal (120s), max dilation of 16 is sufficient
        self.conv_blocks = nn.ModuleList([
            # Block 1: dilation=1, capture fine details
            nn.Sequential(
                nn.Conv1d(1, 64, kernel_size=7, dilation=1, padding='same'),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Dropout(0.2)
            ),
            # Block 2: dilation=2
            nn.Sequential(
                nn.Conv1d(64, 128, kernel_size=7, dilation=2, padding='same'),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.3)
            ),
            # Block 3: dilation=4
            nn.Sequential(
                nn.Conv1d(128, 256, kernel_size=7, dilation=4, padding='same'),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.3)
            ),
            # Block 4: dilation=8
            nn.Sequential(
                nn.Conv1d(256, 256, kernel_size=7, dilation=8, padding='same'),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.4)
            ),
            # Block 5: dilation=16 (final)
            nn.Sequential(
                nn.Conv1d(256, 512, kernel_size=7, dilation=16, padding='same'),
                nn.BatchNorm1d(512),
                nn.ReLU(),
                nn.Dropout(0.4)
            )
        ])

        # Global pooling and classification
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)  # NO sigmoid - raw logits for CrossEntropyLoss
        )

    def forward(self, x):
        # Apply dilated conv blocks
        for block in self.conv_blocks:
            x = block(x)

        # Global pooling and classification
        x = self.global_pool(x)
        x = self.classifier(x)
        return x

class PPGDataset(Dataset):
    """Optimized Dataset class for GPU training"""
    def __init__(self, windows, labels):
        # Convert to tensors immediately
        self.windows = torch.FloatTensor(windows)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.windows)

    def __getitem__(self, idx):
        return self.windows[idx].unsqueeze(0), self.labels[idx]

def load_wesad_data_colab(data_dir="./"):
    """
    Load WESAD data for Colab/Kaggle environment
    Adjust data_dir based on where you uploaded files
    """
    print("Loading WESAD data...")
    data_dir = Path(data_dir)

    all_subjects = []
    all_windows = []
    all_labels = []

    # Look for CSV files
    csv_files = list(data_dir.glob("S*.csv"))
    if not csv_files:
        # Try different common locations
        possible_dirs = ["./data/", "./WESAD_BVP_extracted/", "/content/", "/kaggle/input/"]
        for pdir in possible_dirs:
            csv_files = list(Path(pdir).glob("S*.csv"))
            if csv_files:
                data_dir = Path(pdir)
                break

    print(f"Found {len(csv_files)} CSV files in {data_dir}")

    for subject_file in sorted(csv_files):
        subject_id = subject_file.stem
        print(f"Processing {subject_id}...")

        try:
            # Load CSV
            df = pd.read_csv(subject_file)

            if 'BVP' not in df.columns or 'label' not in df.columns:
                print(f"  Skipping {subject_id}: missing BVP or label columns")
                continue

            # Extract labeled segments
            windows, labels, subject_ids = extract_labeled_segments(
                df['BVP'].values, df['label'].values, subject_id
            )

            if len(windows) > 0:
                all_windows.extend(windows)
                all_labels.extend(labels)
                all_subjects.extend(subject_ids)
                print(f"  {subject_id}: Created {len(windows)} windows")

        except Exception as e:
            print(f"  Error with {subject_id}: {e}")
            continue

    print(f"\nTotal windows: {len(all_windows)}")
    if len(all_labels) > 0:
        print(f"Label distribution: {np.bincount(all_labels)}")

    return np.array(all_windows), np.array(all_labels), np.array(all_subjects)

def extract_labeled_segments(bvp_signal, labels, subject_id, fs=64):
    """Extract windows from labeled segments - SAME as your existing function"""
    window_size = int(120 * fs)  # 120 seconds = 7680 samples
    step_size = int(30 * fs)     # 30 seconds = 1920 samples

    windows = []
    window_labels = []
    subject_ids = []

    # Find labeled segments
    label_series = pd.Series(labels)
    valid_mask = label_series.notna()

    if not valid_mask.any():
        return windows, window_labels, subject_ids

    # Process stress (1.0) and non-stress (0.0) segments
    for label_val in [0.0, 1.0]:
        label_mask = (label_series == label_val) & valid_mask

        if not label_mask.any():
            continue

        # Find continuous segments
        label_indices = np.where(label_mask)[0]

        # Create windows within segments
        for start_idx in range(0, len(label_indices) - window_size + 1, step_size):
            if start_idx + window_size <= len(label_indices):
                # Get indices for this window
                window_indices = label_indices[start_idx:start_idx + window_size]

                # Check if indices are continuous
                if np.all(np.diff(window_indices) == 1):
                    window = bvp_signal[window_indices]

                    # Normalize window (same as your existing preprocessing)
                    window = (window - np.mean(window)) / (np.std(window) + 1e-8)

                    windows.append(window)
                    window_labels.append(int(label_val))
                    subject_ids.append(subject_id)

    return windows, window_labels, subject_ids

def train_dilated_cnn(model, train_loader, device, epochs=30, patience=7):
    """
    Training with improvements:
    - Proper class weighting
    - Learning rate scheduling
    - Early stopping
    """

    # Calculate class weights
    all_labels = []
    for _, labels in train_loader:
        all_labels.extend(labels.numpy())

    class_counts = np.bincount(all_labels)
    total_samples = len(all_labels)
    class_weights = total_samples / (len(class_counts) * class_counts)
    class_weights_tensor = torch.FloatTensor(class_weights).to(device)

    print(f"  Class distribution: {class_counts}")
    print(f"  Class weights: {class_weights}")

    # Setup training
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=False
    )

    best_loss = float('inf')
    patience_counter = 0

    model.train()

    for epoch in range(epochs):
        epoch_loss = 0.0
        epoch_correct = 0
        epoch_total = 0

        for batch_windows, batch_labels in train_loader:
            batch_windows = batch_windows.to(device, non_blocking=True)
            batch_labels = batch_labels.to(device, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(batch_windows)
            loss = criterion(outputs, batch_labels)
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            epoch_total += batch_labels.size(0)
            epoch_correct += (predicted == batch_labels).sum().item()

        avg_loss = epoch_loss / len(train_loader)
        epoch_acc = 100 * epoch_correct / epoch_total

        scheduler.step(avg_loss)

        # Print progress every 5 epochs
        if (epoch + 1) % 5 == 0:
            print(f'  Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Acc={epoch_acc:.2f}%')

        # Early stopping
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'best_dilated_model.pth')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'  Early stopping at epoch {epoch+1}')
                break

        # Memory cleanup
        torch.cuda.empty_cache()

    # Load best model
    model.load_state_dict(torch.load('best_dilated_model.pth'))
    return model

def run_cnn_loso(windows, labels, subjects):
    """CNN LOSO cross-validation - matches your exact evaluation methodology"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Match the exact subject list from your hybrid code
    target_subjects = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17]

    # Filter to only subjects that exist in our data
    available_subjects = []
    for subj in target_subjects:
        # Try both "S2" and "S02" formats
        subj_str1 = f"S{subj}"      # "S2", "S3", etc.
        subj_str2 = f"S{subj:02d}"  # "S02", "S03", etc.

        if subj_str1 in subjects:
            available_subjects.append(subj_str1)
        elif subj_str2 in subjects:
            available_subjects.append(subj_str2)

    print(f"Target subjects from hybrid: {target_subjects}")
    print(f"Available in data: {sorted(np.unique(subjects))}")
    print(f"Running CNN LOSO CV on {len(available_subjects)} subjects: {available_subjects}")

    if len(available_subjects) != 15:
        print(f"WARNING: Expected 15 subjects but only found {len(available_subjects)}")

    all_results = []

    for fold, test_subject in enumerate(available_subjects):
        print(f"\n=== CNN Fold {fold+1}/{len(available_subjects)}: Testing {test_subject} ===")

        # Split data - EXACT same as your hybrid code
        test_mask = subjects == test_subject
        train_mask = ~test_mask

        X_train, X_test = windows[train_mask], windows[test_mask]
        y_train, y_test = labels[train_mask], labels[test_mask]

        print(f"Train: {len(X_train)} samples, Test: {len(X_test)} samples")
        print(f"Train distribution: {np.bincount(y_train)}")
        print(f"Test distribution: {np.bincount(y_test)}")

        # NOTE: Preprocessing already done in extract_labeled_segments
        # Windows are already normalized, so use them directly

        # Create datasets
        train_dataset = PPGDataset(X_train, y_train)
        test_dataset = PPGDataset(X_test, y_test)

        # Create loaders
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,
                                 num_workers=2, pin_memory=True)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False,
                                num_workers=2, pin_memory=True)

        # Initialize model
        model = FixedDilatedCNN().to(device)

        # Train
        start_time = time.time()
        model = train_dilated_cnn(model, train_loader, device, epochs=30, patience=7)
        train_time = time.time() - start_time

        # Test
        model.eval()
        test_preds = []
        test_probs = []
        test_true = []

        with torch.no_grad():
            for batch_windows, batch_labels in test_loader:
                batch_windows = batch_windows.to(device)
                outputs = model(batch_windows)
                probs = torch.softmax(outputs, dim=1)
                _, preds = torch.max(outputs, 1)

                test_preds.extend(preds.cpu().numpy())
                test_probs.extend(probs.cpu().numpy())
                test_true.extend(batch_labels.numpy())

        # Calculate EXACT same metrics as your hybrid code
        test_preds = np.array(test_preds)
        test_probs = np.array(test_probs)
        test_true = np.array(test_true)

        # Match hybrid metric calculation exactly
        accuracy = accuracy_score(test_true, test_preds) * 100  # Convert to percentage

        if len(np.unique(test_true)) > 1:
            auc = roc_auc_score(test_true, test_preds) * 100  # Use predictions for consistency
        else:
            auc = 0.0

        f1 = f1_score(test_true, test_preds) * 100  # Convert to percentage

        # Extract subject number for comparison
        if test_subject.startswith('S'):
            subj_num = int(test_subject[1:])
        else:
            subj_num = int(test_subject)

        result = {
            'fold': fold + 1,
            'subject': subj_num,
            'test_subject': test_subject,
            'accuracy': accuracy,
            'auc_roc': auc,
            'f1_score': f1,
            'train_time': train_time
        }

        all_results.append(result)
        print(f"CNN Results: Acc={accuracy:.2f}%, AUC={auc:.2f}%, F1={f1:.2f}%")
        print(f"Training time: {train_time:.1f}s")

        # Cleanup
        del model
        torch.cuda.empty_cache()
        gc.collect()

    return all_results

def save_cnn_results(results):
    """Save CNN results in format compatible with hybrid comparison"""

    # Convert to DataFrame
    results_df = pd.DataFrame(results)

    # Calculate summary statistics
    mean_accuracy = results_df['accuracy'].mean()
    std_accuracy = results_df['accuracy'].std()
    mean_auc = results_df['auc_roc'].mean()
    std_auc = results_df['auc_roc'].std()
    mean_f1 = results_df['f1_score'].mean()
    std_f1 = results_df['f1_score'].std()
    mean_time = results_df['train_time'].mean()

    print(f"\n=== CNN LOSO Cross-Validation Results ===")
    print(f"Mean Accuracy: {mean_accuracy:.2f}% ± {std_accuracy:.2f}%")
    print(f"Mean AUC-ROC:  {mean_auc:.2f}% ± {std_auc:.2f}%")
    print(f"Mean F1-Score: {mean_f1:.2f}% ± {std_f1:.2f}%")
    print(f"Mean Training Time: {mean_time:.1f}s per fold")

    # Save detailed results
    results_df.to_csv('cnn_loso_results.csv', index=False)
    print(f"Detailed results saved to 'cnn_loso_results.csv'")

    return results_df

# ===== MAIN EXECUTION =====
def main():
    print("=== WESAD CNN Classification (Fixed Dilated CNN) ===")
    print("Comparing against TD+HRV feature approach")

    # Load data - SAME function as your hybrid code
    data_dir = "./WESAD_BVP_extracted/"  # Adjust path as needed
    windows, labels, subjects = load_wesad_data_colab(data_dir)

    if len(windows) == 0:
        print("No data found! Check your data directory.")
        return

    print(f"Loaded {len(windows)} windows from {len(np.unique(subjects))} subjects")
    print(f"Available subjects: {sorted(np.unique(subjects))}")

    # Run CNN LOSO cross-validation (matching hybrid exactly)
    cnn_results = run_cnn_loso(windows, labels, subjects)

    # Save results for comparison with hybrid
    cnn_results_df = save_cnn_results(cnn_results)

    print("\n=== CNN Pipeline Complete ===")
    print("Results can now be directly compared with hybrid TD+CNN features!")

    return cnn_results_df

# Run the main function
if __name__ == "__main__":
    main()